from __future__ import annotations

import sys
from datetime import datetime, timezone
from functools import lru_cache
from pathlib import Path
from urllib.parse import urlparse

from sqlalchemy import (
    Boolean,
    Column,
    DateTime,
    Index,
    Integer,
    String,
    Text,
    create_engine,
    event,
)
from sqlalchemy.orm import declarative_base, sessionmaker

from news_crawler.core.settings import get_settings

Base = declarative_base()


class NewsArticle(Base):
    __tablename__ = "raw_news"

    id = Column(Integer, primary_key=True)
    title = Column(String(512), nullable=False)
    link = Column(String(1024), unique=True, nullable=False)
    content_hash = Column(String(64), index=True)
    content_text = Column(Text)
    source = Column(String(50))
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc), index=True)

    summary = Column(Text, nullable=True)
    # ğŸ”¥ ä½¿ç”¨ Text ç±»å‹é˜²æ­¢ AI ç”Ÿæˆçš„æ ‡ç­¾è¿‡å¤šå¯¼è‡´ Data too long é”™è¯¯
    ai_tags = Column(Text, nullable=True)
    is_ai_processed = Column(Boolean, default=False, index=True)
    category = Column(String(50), index=True, nullable=True)
    importance_score = Column(Integer, default=0, index=True)

    # å¤åˆç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢
    __table_args__ = (
        # AIå¤„ç†æŸ¥è¯¢ï¼šæŸ¥æ‰¾æœªå¤„ç†çš„æ–‡ç« 
        Index('ix_raw_news_ai_pending', 'is_ai_processed', postgresql_where=is_ai_processed.is_(False)),
        # æŠ¥è¡¨æŸ¥è¯¢ï¼šæŒ‰åˆ†ç±»+æ—¶é—´+åˆ†æ•°æŸ¥è¯¢
        Index('ix_raw_news_report', 'category', 'created_at', 'importance_score'),
    )


@lru_cache(maxsize=1)
def get_engine():
    settings = get_settings()
    db_uri = settings.db.build_uri()
    if not db_uri:
        raise RuntimeError(
            "Database is not configured. Please set DB_URI or (DB_HOST/DB_USER/DB_PASS/DB_PORT)."
        )

    parsed = urlparse(db_uri)
    is_sqlite = parsed.scheme.startswith("sqlite")

    if is_sqlite:
        # å°è¯•åˆ›å»º SQLite æ–‡ä»¶ç›®å½•ï¼ˆ:memory: ä¸å¤„ç†ï¼‰
        if parsed.path and parsed.path not in ("/", "/:memory:") and ":memory:" not in db_uri:
            # sqlite ç»å¯¹è·¯å¾„é€šå¸¸å½¢å¦‚ sqlite:////abs/path.db æˆ– sqlite+pysqlite:////abs/path.db
            # urlparse ä¼šæŠŠè·¯å¾„æ”¾åœ¨ .pathï¼Œå»æ‰å‰å¯¼ '/' åå†å½“ä½œæ–‡ä»¶è·¯å¾„å¤„ç†
            candidate = parsed.path
            # å¯¹äº sqlite:////abs/path.dbï¼Œparsed.path ä¸º '//abs/path.db'
            while candidate.startswith("/"):
                candidate = candidate[1:]
            if candidate:
                path = Path("/" + candidate)
                path.parent.mkdir(parents=True, exist_ok=True)

        engine = create_engine(
            db_uri,
            pool_pre_ping=True,
            connect_args={"check_same_thread": False},
        )

        # ğŸ”¥ SQLite æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒé€»è¾‘
        # å¼€å¯ WAL (Write-Ahead Logging) æ¨¡å¼ï¼Œå¤§å¹…æå‡å¹¶å‘å†™å…¥æ€§èƒ½
        @event.listens_for(engine, "connect")
        def set_sqlite_pragma(dbapi_connection, connection_record):
            cursor = dbapi_connection.cursor()
            cursor.execute("PRAGMA journal_mode=WAL")
            # NORMAL æ¨¡å¼åœ¨ WAL ä¸‹æ˜¯å®‰å…¨çš„ï¼Œä¸”æ¯” FULL å¿«å¾ˆå¤š
            cursor.execute("PRAGMA synchronous=NORMAL")
            cursor.close()

        # SQLite æ¨¡å¼ä¸‹è‡ªåŠ¨å»ºè¡¨ï¼Œå®ç°â€œå¼€ç®±å³ç”¨â€
        Base.metadata.create_all(engine)
        return engine

    # PostgreSQL / å…¶ä»–æ•°æ®åº“
    return create_engine(
        db_uri,
        pool_size=5,
        pool_recycle=1800,
        pool_pre_ping=True,
        connect_args={"connect_timeout": 10},
    )


def _try_create_sessionmaker():
    try:
        engine = get_engine()
    except Exception:
        return None
    return sessionmaker(bind=engine, autoflush=False, expire_on_commit=False)


SessionLocal = _try_create_sessionmaker()

try:
    engine = get_engine()
except Exception:
    engine = None


if __name__ == "__main__":
    import logging

    from news_crawler.core.bootstrap import bootstrap

    logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')
    logger = logging.getLogger(__name__)

    bootstrap()
    logger.info("Testing database connection...")
    try:
        engine = get_engine()
        Base.metadata.create_all(engine)
        logger.info("âœ“ Database connection successful, tables synced!")

        # ç®€å•æ£€æŸ¥ WAL æ˜¯å¦ç”Ÿæ•ˆ (ä»…é’ˆå¯¹ SQLite)
        if str(engine.url).startswith("sqlite"):
            with engine.connect() as conn:
                mode = conn.exec_driver_sql("PRAGMA journal_mode").scalar()
                logger.info(f"SQLite Journal Mode: {mode} (Expected: wal)")

    except Exception as e:
        logger.error(f"Connection failed: {e}")
        sys.exit(1)
