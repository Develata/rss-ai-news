from datetime import datetime, timedelta, timezone
from zoneinfo import ZoneInfo

from news_crawler.core.config import REPORT_CONFIGS
from news_crawler.core.database import NewsArticle
from news_crawler.services.ai_service import get_custom_ai_response
from news_crawler.services.publisher_service import GitHubPublisher

try:
    from news_crawler.utils.logger import logger
except ImportError:
    import logging

    logger = logging.getLogger(__name__)


def generate_excerpt(articles, title_prefix):
    if not articles:
        return "æœ¬æœŸæ— å†…å®¹ã€‚"

    titles_list = "\n".join([f"- {art.title}" for art in articles[:15]])

    system_prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªç§‘æŠ€æ–°é—»ä¸»ç¼–ã€‚è¯·æ ¹æ®ä»¥ä¸‹ã€{title_prefix}ã€‘æ¿å—çš„æ–°é—»æ ‡é¢˜åˆ—è¡¨ï¼Œ"
        "å†™ä¸€æ®µç®€çŸ­çš„æ—¥æŠ¥å¯¼è¯»ï¼ˆExcerptï¼‰ã€‚è¦æ±‚è¯­æ°”ä¸“ä¸šå®¢è§‚ï¼Œ80å­—ä»¥å†…ã€‚"
    )

    try:
        excerpt = get_custom_ai_response(titles_list, system_prompt)
        if not excerpt:
            return "ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹é€Ÿè§ˆã€‚"
        return excerpt.replace('"', "").replace("'", "")
    except Exception as e:
        logger.warning(f"âš ï¸ ç”Ÿæˆå¯¼è¯»å¤±è´¥: {e}")
        return "ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹é€Ÿè§ˆã€‚"


def generate_md_content(articles, config):
    if not articles:
        return None

    tz = ZoneInfo("Asia/Shanghai")
    now = datetime.now(tz)
    date_str = f"{now.year}-{now.month}-{now.day}"

    excerpt_text = generate_excerpt(articles, config["title_prefix"])
    if not excerpt_text:
        excerpt_text = "æš‚æ— æ‘˜è¦"

    raw_title = f"{config['title_prefix']} {date_str}"

    safe_title = raw_title.replace('"', "\\\"").strip()
    safe_excerpt = excerpt_text.replace('"', "\\\"").replace("\n", " ").strip()

    md = [
        "---",
        f'title: "{safe_title}"',
        f"date: {date_str}",
        f'excerpt: "{safe_excerpt}"',
        "---",
        "",
        f"# {safe_title}\n",
        f"> {excerpt_text}\n",
    ]

    for art in articles:
        title = (
            art.title.replace("|", "-")
            .replace("[", "(")
            .replace("]", ")")
            .strip()
        )

        tags = "".join(
            [f"`{t.strip()}` " for t in (art.ai_tags or "").split(",") if t.strip()]
        )

        md.append(
            f"## {title} <Badge type=\"tip\" text=\"{art.importance_score}\" />\n"
        )
        if tags:
            md.append(f"- **Tags:** {tags}\n")

        md.append(f"- **Source:** `{art.source}` | [é˜…è¯»åŽŸæ–‡]({art.link})\n")

        summary_text = art.summary if art.summary else "æš‚æ— æ‘˜è¦"
        md.append(f"> {summary_text}\n\n")
        md.append("---\n")

    return "\n".join(md)


def run_publishing_job(session):
    publisher = GitHubPublisher()
    published_count = 0

    tz = ZoneInfo("Asia/Shanghai")
    now = datetime.now(tz)
    current_year = str(now.year)
    current_date_file = now.strftime("%Y%m%d")

    time_window = datetime.now(timezone.utc) - timedelta(hours=25)
    
    # ä¼˜åŒ–ï¼šä¸€æ¬¡æŸ¥è¯¢èŽ·å–æ‰€æœ‰åˆ†ç±»çš„æ–‡ç« ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”
    all_articles = (
        session.query(NewsArticle)
        .filter(
            NewsArticle.created_at >= time_window,
            NewsArticle.is_ai_processed == True,
            NewsArticle.category.in_(list(REPORT_CONFIGS.keys())),
        )
        .order_by(
            NewsArticle.category,
            NewsArticle.importance_score.desc(),
            NewsArticle.created_at.desc(),
        )
        .all()
    )
    
    # æŒ‰åˆ†ç±»åˆ†ç»„
    articles_by_category = {}
    for art in all_articles:
        if art.category not in articles_by_category:
            articles_by_category[art.category] = []
        if len(articles_by_category[art.category]) < 10:  # æ¯ä¸ªåˆ†ç±»æœ€å¤š10æ¡
            articles_by_category[art.category].append(art)

    for category_key, cfg in REPORT_CONFIGS.items():
        try:
            articles = articles_by_category.get(category_key, [])

            if articles:
                logger.info(
                    f"    âœ… Generating {cfg['title_prefix']} ({len(articles)} items)"
                )
                content = generate_md_content(articles, cfg)

                folder_name = cfg.get("folder", "Other")
                file_path = f"{folder_name}/{current_year}/{current_date_file}.md"

                publisher.push_markdown(file_path, content)
                published_count += 1
            else:
                logger.info(
                    f"    ðŸ˜´ Skipped {cfg['title_prefix']} (No content processed today)"
                )

        except Exception as e:
            logger.error(f"    âŒ Error generating report for [{category_key}]: {e}")
            continue

    return published_count
